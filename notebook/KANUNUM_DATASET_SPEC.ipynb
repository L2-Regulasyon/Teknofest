{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KANUNUM_DATASET_SPEC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkrCl6__jOtt"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install pandas nb_black pytorch-lightning torch tokenizers transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import inspect\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "from collections import Iterable\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import tokenizers\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit\n",
        "\n",
        "#%load_ext lab_black\n",
        "\n",
        "pd.options.display.max_colwidth = 250\n",
        "pd.options.display.max_columns = 250"
      ],
      "metadata": {
        "id": "USN-HM8IjRfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_DIR = Path(\"data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "(DATA_DIR / \"raw\").mkdir(exist_ok=True)\n",
        "\n",
        "!cp drive/MyDrive/comp_data/openhack/kanunum-nlp-doc-analysis-dataset.csv data/raw"
      ],
      "metadata": {
        "id": "QPad_RPYjS30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from inspect import signature\n",
        "\n",
        "class KanunumClassificationDataset(Dataset):\n",
        "    MAPPINGS = {'cumhurbaşkanlığı kararnamesi': 0,\n",
        "                'genelge': 1,\n",
        "                'kanun': 2,\n",
        "                'kanun hükmünde kararname': 3,\n",
        "                'komisyon raporu': 4,\n",
        "                'resmi gazete': 5,\n",
        "                'tebliğ': 6,\n",
        "                'tüzük': 7,\n",
        "                'yönetmelik': 8,\n",
        "                'özelge': 9}\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        df,\n",
        "        text_field=\"baslik\",\n",
        "        text_pair=None,\n",
        "        target_field=\"kategori\",\n",
        "        pretrained_model_name_or_path=\"dbmdz/convbert-base-turkish-mc4-uncased\",\n",
        "        encode_pair=True,\n",
        "        tokenizer_init_kwargs=None,\n",
        "        tokenizer_encode_kwargs=None,\n",
        "        return_params=None,\n",
        "        is_inference=True,\n",
        "    ):\n",
        "        \n",
        "        assert isinstance(text_field, str) and text_field in df.columns\n",
        "\n",
        "        self.labels = None\n",
        "        self.num_class = None\n",
        "\n",
        "        if not is_inference:\n",
        "            assert target_field in df.columns\n",
        "\n",
        "            df[target_field] = df[target_field].str.lower().str.strip()\n",
        "            \n",
        "            self.num_class = len(self.MAPPINGS.values())\n",
        "            \n",
        "            labels = np.zeros(shape=(df.shape[0], self.num_class))\n",
        "            for row, col in df[target_field].map(self.MAPPINGS).to_frame(\"col\").reset_index().values:\n",
        "                labels[row, col] = 1\n",
        "\n",
        "            self.labels = pd.DataFrame(labels)\n",
        "            \n",
        "\n",
        "        is_pair_encoded = False\n",
        "        tokenization_base_kwargs = {\n",
        "            \"return_attention_mask\": True,\n",
        "            \"return_token_type_ids\": True,\n",
        "        }\n",
        "\n",
        "        if not tokenizer_encode_kwargs:\n",
        "            tokenizer_encode_kwargs = {}\n",
        "\n",
        "        assert isinstance(tokenizer_encode_kwargs, dict)\n",
        "        tokenizer_encode_kwargs.update(tokenization_base_kwargs)\n",
        "\n",
        "        self.return_params = [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n",
        "\n",
        "        if encode_pair and text_pair:\n",
        "            assert isinstance(text_pair, str) and text_pair in df.columns\n",
        "            is_pair_encoded = True\n",
        "            # self.text_field2 = text_pair\n",
        "\n",
        "        self.is_pair_encoded = is_pair_encoded\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n",
        "\n",
        "        encoding_params = signature(self.tokenizer.encode_plus).parameters.keys()\n",
        "\n",
        "        self.tokenizer_encode_kwargs = {\n",
        "            key: val\n",
        "            for key, val in tokenizer_encode_kwargs.items()\n",
        "            if key in encoding_params\n",
        "        }\n",
        "        # self.text_field1 = text_field\n",
        "\n",
        "        self.df = df.loc[\n",
        "            :, [text_field] + ([text_pair] if self.is_pair_encoded else [])\n",
        "        ].reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        tokenization_info = self.tokenizer(\n",
        "            *self.df.loc[idx, :].values, **self.tokenizer_encode_kwargs\n",
        "        )\n",
        "\n",
        "        if isinstance(self.labels, pd.DataFrame):\n",
        "            label = self.labels.loc[idx, :].values\n",
        "            label = torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "            return {\n",
        "                param: torch.tensor(tokenization_info[param], dtype=torch.long)\n",
        "                for param in self.return_params\n",
        "            }, label\n",
        "\n",
        "        return {\n",
        "                param: torch.tensor(tokenization_info[param], dtype=torch.long)\n",
        "                for param in self.return_params\n",
        "            }"
      ],
      "metadata": {
        "id": "s3kEgqeujVXG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}