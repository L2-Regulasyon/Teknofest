{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KANUNUM_DATAMODULE_SPEC.ipynb.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO8gCOL6ai28oxVyi5J7F67"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"dLuIqQyAjf-q"},"outputs":[],"source":["import sys\n","!{sys.executable} -m pip install pandas nb_black pytorch-lightning torch tokenizers transformers"]},{"cell_type":"code","source":["import sys\n","import inspect\n","import pathlib\n","from pathlib import Path\n","from collections import Iterable\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils.data import DataLoader, Dataset\n","\n","import tokenizers\n","from transformers import AutoModel, AutoTokenizer\n","\n","import pytorch_lightning as pl\n","\n","from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit\n","\n","#%load_ext lab_black\n","\n","pd.options.display.max_colwidth = 250\n","pd.options.display.max_columns = 250"],"metadata":{"id":"tjQ2CnvTjkBQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","DATA_DIR = Path(\"data\")\n","DATA_DIR.mkdir(exist_ok=True)\n","(DATA_DIR / \"raw\").mkdir(exist_ok=True)\n","\n","!cp drive/MyDrive/comp_data/openhack/kanunum-nlp-doc-analysis-dataset.csv data/raw"],"metadata":{"id":"P5ZhHa18jkwm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","from inspect import signature\n","\n","class KanunumClassificationDataset(Dataset):\n","    MAPPINGS = {'cumhurbaşkanlığı kararnamesi': 0,\n","                'genelge': 1,\n","                'kanun': 2,\n","                'kanun hükmünde kararname': 3,\n","                'komisyon raporu': 4,\n","                'resmi gazete': 5,\n","                'tebliğ': 6,\n","                'tüzük': 7,\n","                'yönetmelik': 8,\n","                'özelge': 9}\n","\n","\n","    def __init__(\n","        self,\n","        df,\n","        text_field=\"baslik\",\n","        text_pair=None,\n","        target_field=\"kategori\",\n","        pretrained_model_name_or_path=\"dbmdz/convbert-base-turkish-mc4-uncased\",\n","        encode_pair=True,\n","        tokenizer_init_kwargs=None,\n","        tokenizer_encode_kwargs=None,\n","        return_params=None,\n","        is_inference=True,\n","    ):\n","        \n","        assert isinstance(text_field, str) and text_field in df.columns\n","\n","        self.labels = None\n","        self.num_class = None\n","\n","        if not is_inference:\n","            assert target_field in df.columns\n","\n","            df[target_field] = df[target_field].str.lower().str.strip()\n","            \n","            self.num_class = len(self.MAPPINGS.values())\n","            \n","            labels = np.zeros(shape=(df.shape[0], self.num_class))\n","            for row, col in df[target_field].map(self.MAPPINGS).to_frame(\"col\").reset_index().values:\n","                labels[row, col] = 1\n","\n","            self.labels = pd.DataFrame(labels)\n","            \n","\n","        is_pair_encoded = False\n","        tokenization_base_kwargs = {\n","            \"return_attention_mask\": True,\n","            \"return_token_type_ids\": True,\n","        }\n","\n","        if not tokenizer_encode_kwargs:\n","            tokenizer_encode_kwargs = {}\n","\n","        assert isinstance(tokenizer_encode_kwargs, dict)\n","        tokenizer_encode_kwargs.update(tokenization_base_kwargs)\n","\n","        self.return_params = [\"input_ids\", \"attention_mask\", \"token_type_ids\"]\n","\n","        if encode_pair and text_pair:\n","            assert isinstance(text_pair, str) and text_pair in df.columns\n","            is_pair_encoded = True\n","            # self.text_field2 = text_pair\n","\n","        self.is_pair_encoded = is_pair_encoded\n","        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path)\n","\n","        encoding_params = signature(self.tokenizer.encode_plus).parameters.keys()\n","\n","        self.tokenizer_encode_kwargs = {\n","            key: val\n","            for key, val in tokenizer_encode_kwargs.items()\n","            if key in encoding_params\n","        }\n","        # self.text_field1 = text_field\n","\n","        self.df = df.loc[\n","            :, [text_field] + ([text_pair] if self.is_pair_encoded else [])\n","        ].reset_index(drop=True)\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","\n","        tokenization_info = self.tokenizer(\n","            *self.df.loc[idx, :].values, **self.tokenizer_encode_kwargs\n","        )\n","\n","        if isinstance(self.labels, pd.DataFrame):\n","            label = self.labels.loc[idx, :].values\n","            label = torch.tensor(label, dtype=torch.float32)\n","\n","            return {\n","                param: torch.tensor(tokenization_info[param], dtype=torch.long)\n","                for param in self.return_params\n","            }, label\n","\n","        return {\n","                param: torch.tensor(tokenization_info[param], dtype=torch.long)\n","                for param in self.return_params\n","            }"],"metadata":{"id":"jGIoek-6jlbj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class KanunumDataModule(pl.LightningDataModule):\n","    DATA_FIELDS = ['id', 'kategori', 'baslik', 'rega_no', 'mukerrer_no', 'rega_tarihi',\n","                  'kurum', 'mevzuat_no', 'belge_sayi', 'mevzuat_tarihi', 'donem',\n","                  'sira_no', 'madde_sayisi', 'data_text', 'url', 'kanunum_url']\n","\n","    def __init__(self, \n","                 data_dir: pathlib.Path,\n","                 data_prefix=\"kanunum-nlp-doc-analysis*.csv\",\n","                 label=\"kategori\",\n","                 text_field='baslik',\n","                 train_indices=None,\n","                 test_indices=None,\n","                 split_kwargs=None,\n","                 test_split_kwargs=None,\n","                 pretrained_model_name_or_path='dbmdz/convbert-base-turkish-mc4-uncased',\n","                 text_pair=None,   \n","                 encode_pair=True, \n","                 tokenizer_init_kwargs=None, \n","                 tokenizer_encode_kwargs=None, \n","                 return_params=None, \n","                 batch_size=256,\n","                 num_workers=0,\n","                 drop_last=False,\n","                 data_loader_kwargs=None,\n","                 errors=\"raise\"\n","                 ):\n","      \n","        assert errors in (\"raise\", \"ignore\")\n","        assert not all([test_split_kwargs is not None, test_indices is not None])\n","\n","        if not split_kwargs:\n","            split_kwargs = {}\n","\n","        if not data_loader_kwargs:\n","            data_loader_kwargs = {}\n","\n","        assert isinstance(split_kwargs, dict)\n","\n","        super().__init__()\n","\n","        self.data_dir = data_dir\n","        self.data_prefix = data_prefix\n","        self.dataset_kwargs = {\"pretrained_model_name_or_path\": pretrained_model_name_or_path,\n","                               \"text_field\": text_field,\n","                               \"text_pair\": text_pair,\n","                               \"encode_pair\": encode_pair,\n","                               \"tokenizer_init_kwargs\": tokenizer_init_kwargs,\n","                               \"tokenizer_encode_kwargs\": tokenizer_encode_kwargs,\n","                               \"return_params\": return_params,\n","                                }\n","\n","        self.data_loader_kwargs = {**{\"drop_last\": drop_last,\n","                                      \"num_workers\": num_workers,\n","                                      \"batch_size\": batch_size,\n","                                      },\n","                                      **data_loader_kwargs\n","                                   }\n","        self.label = label\n","        \n","        self.train_indices = train_indices\n","        self.test_indices = test_indices\n","\n","        self.split_kwargs = split_kwargs\n","        self.test_split_kwargs = test_split_kwargs\n","\n","        self.include_test_split = any([arg is not None for arg in [train_indices, test_indices]]) or test_split_kwargs\n","        self.errors = errors\n","        \n","\n","    def prepare_data(self):\n","        self.df = self.__class__._read_and_merge(self.data_dir, self.data_prefix)\n","\n","    def setup(self, stage = None):\n","        self.prepare_data()\n","\n","        if stage == \"fit\" or stage is None:\n","            assert self.label in self.df.columns\n","\n","            (train, val), _ = self.__class__._construct_test_indices(self.df, \n","                                                                      label=self.label, \n","                                                                      test_split_kwargs=self.test_split_kwargs,\n","                                                                      split_kwargs=self.split_kwargs, \n","                                                                      include_test_split=self.include_test_split,\n","                                                                      train_indices = self.train_indices,\n","                                                                      test_indices = self.test_indices,\n","                                                                      errors=self.errors)\n","            \n","            self.train = KanunumClassificationDataset(train, **{**self.dataset_kwargs, **{\"is_inference\": False}})\n","            self.val = KanunumClassificationDataset(val, **{**self.dataset_kwargs, **{\"is_inference\": False}})\n","\n","        if stage in (\"test\", \"predict\") or stage is None:\n","            assert self.include_test_split\n","            _, test_df = self.__class__._construct_test_indices(self.df, \n","                                                                label=self.label, \n","                                                                test_split_kwargs=self.test_split_kwargs,\n","                                                                split_kwargs=self.split_kwargs, \n","                                                                train_indices = self.train_indices,\n","                                                                test_indices = self.test_indices,\n","                                                                include_test_split=self.include_test_split,\n","                                                                errors=self.errors)\n","\n","            self.test = KanunumClassificationDataset(test_df, **{**self.dataset_kwargs, **{\"is_inference\": True}})\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.train, **self.data_loader_kwargs)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val, **self.data_loader_kwargs)\n","\n","    def test_dataloader(self):\n","        return DataLoader(self.test, **self.data_loader_kwargs)\n","\n","    def predict_dataloader(self):\n","        return DataLoader(self.test, **self.data_loader_kwargs)\n","\n","        \n","    @classmethod\n","    def _construct_fold_indices(cls, \n","                                df,\n","                                label,  \n","                                split_signature,\n","                                split_kwargs,\n","                                include_test_split=True,\n","                                errors=\"raise\"\n","                                ):\n","        \n","      if errors == \"ignore\":    \n","        signature_args = inspect.signature(split_signature).parameters.keys()\n","        split_kwargs = {key:val for key, val in split_kwargs.items() if key in signature_args}\n","\n","      cv = split_signature(**split_kwargs)\n","\n","      if include_test_split:\n","        _, test_indices = train_test_split(df.index, test_size=test_size, random_state=random_state, shuffle=shuffle, stratify=df[label])\n","        test_df = df.loc[test_indices].reset_index(drop=True)\n","        df = df.loc[df.index.difference(test_indices)].reset_index(drop=True)\n","\n","      index_id_mappings = dict(df.reset_index().loc[:, [\"index\", \"id\"]].values)\n","\n","      fold_indices = pd.concat([pd.concat([\n","                  pd.DataFrame({\"fold\": fold_idx, \"indices\": train_indices, \"is_train\": True}),\n","                  pd.DataFrame({\"fold\": fold_idx, \"indices\": test_indices, \"is_train\": False}),\n","                  ]\n","                )\n","      for fold_idx, (train_indices, test_indices) in enumerate(list(cv.split(df.index, df[label])))])\n","      fold_indices[\"id\"] = fold_indices[\"indices\"].map(index_id_mappings)\n","\n","      return fold_indices, test_df\n","\n","    \n","    @classmethod\n","    def _construct_test_indices(cls,\n","                                df,\n","                                label,\n","                                test_split_kwargs=None,\n","                                split_kwargs=None,\n","                                train_indices=None,\n","                                test_indices=None,\n","                                include_test_split=False,\n","                                errors=\"raise\",\n","                                #split_signature=None,\n","                                ):\n","      \n","        from sklearn.model_selection import train_test_split\n","      \n","        is_train_idx_provided, is_test_idx_provided = [arg is not None for arg in [train_indices, test_indices]]\n","        is_indices_pre_provided = any([is_train_idx_provided, is_test_idx_provided])\n","\n","        test_df = None\n","\n","        if not test_split_kwargs:\n","            test_split_kwargs = {}\n","\n","        if not split_kwargs:\n","            split_kwargs = {}\n","        \n","\n","        if include_test_split:\n","            if not is_test_idx_provided:\n","              _, test_indices = train_test_split(df.index, stratify=df[label], **test_split_kwargs)\n","            \n","            test_df = df.loc[test_indices]\n","            df = df.loc[df.index.difference(test_indices)].reset_index(drop=True)\n","\n","        if is_indices_pre_provided:\n","            train_indices = train_indices if is_train_idx_provided else df.index.difference(test_indices)\n","            \n","        else:\n","            split_signature = train_test_split\n","        \n","            if errors == \"ignore\":    \n","                signature_args = inspect.signature(split_signature).parameters.keys()\n","                split_kwargs = {key:val for key, val in split_kwargs.items() if key in signature_args}\n","\n","            train_indices, _ = split_signature(df.index, stratify=df[label], **split_kwargs)\n","\n","        train = df.loc[train_indices]\n","        val = df.loc[~df.index.isin(train_indices)].reset_index(drop=True)\n","        train = train.reset_index(drop=True)\n","\n","        return (train, val), test_df\n","\n","    @classmethod\n","    def _read_and_merge(cls,\n","                        path: pathlib.Path,\n","                        glob: str):\n","        res = []\n","\n","        assert isinstance(glob, str)\n","        assert glob.endswith(\"csv\") \n","\n","        for path in path.glob(glob):\n","            df = pd.read_csv(path)\n","            df.columns = df.columns.str.lower()\n","            if not len(df.columns.intersection(cls.DATA_FIELDS)) >= len(cls.DATA_FIELDS):\n","              continue\n","            \n","            res.append(df.loc[:, cls.DATA_FIELDS])\n","\n","        res = pd.concat(res)\n","\n","        return res"],"metadata":{"id":"YcA5bn8cjpfA"},"execution_count":null,"outputs":[]}]}